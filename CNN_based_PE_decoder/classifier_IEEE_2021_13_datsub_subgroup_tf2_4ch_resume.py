# -*- coding: utf-8 -*-
"""classifier_zeroshot_tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15mDDi8VSjfED7GWZuIx8AVZQrWGR8OBd
"""
print("hi")
from scipy import signal
import scipy
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import scipy.io as sio
import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import os
from tqdm import tqdm
import pickle
import pdb
import mat73
from datetime import datetime

#pdb.set_trace()

tf.device('/device:GPU:0')
tf.reset_default_graph()


# tf.debugging.set_log_device_placement(True)

def digitize_tolist(arr, bin_num=1024):  # 5
    return np.digitize(arr, np.arange(np.min(arr), np.max(arr), (np.max(arr) - np.min(arr)) / bin_num)).tolist()


def digitize_tolist_(arr, bin_num=1024):  # 5
    asdf = np.digitize(arr, np.arange(-1, 1, (1 - (-1)) / bin_num))
    return asdf.tolist()


def digitize_tolist_dist(arr, bin_num=1024):  # 5
    bx = np.linspace(0, 100, bin_num + 1)
    bxx = []

    for bxi in bx:
        bxx.append(np.percentile(arr, bxi))
    bxx[-1] = bxx[-1] + 1
    bxx[0] = bxx[0] - 1
    return np.digitize(arr, bxx).tolist()


def ext_spectrogram(epoch, fs=1000, window='hamming', nperseg=2000, noverlap=1975, nfft=3000):
    # epoch.shape = channel number, timepoint, trials
    # extract sepctrogram with time point

    dat = []
    for i in range(epoch.shape[2]):
        tfreq = []
        for j in range(epoch.shape[0]):
            f, t, Sxx = signal.stft(epoch[j, :, i], fs=fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft)
            interval = f[-1] / (len(f) - 1)
            req_len = int(40 / interval)
            # use frequency(~121th) and tiem(-41th~0)
            tfreq.append(np.abs(Sxx[:121, -41:]).transpose())
            # tfreq.append(np.abs(Sxx[:121, :]).transpose())

        dat.append(np.asarray(tfreq))

    return np.array(dat)  # shape : (trials, channel number, time, freq), time and freq should be : 41, 121


def get_batch_num(data, batch_size):
    total_len = data.shape[0]
    return math.ceil(total_len / batch_size)


def get_batch(data, batch_size, idx):
    batch_num = get_batch_num(data, batch_size)
    if idx == batch_num - 1:  # last batch
        return data[batch_size * idx:]
    else:
        return data[batch_size * idx:batch_size * (idx + 1)]


# input: location
# output: train_x, train_y, test_x, test_y

def load_data_labels(location='dataset_original2.mat'):
    # load eeg data
    try:
        data = sio.loadmat(location)
    except:
        data = mat73.loadmat(location)

    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2],-1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb_maxrel = data['lb_maxrel'].T
    lb_pmb28 = data['lb_pmb28'].T
    lb_pmb37 = data['lb_pmb37'].T
    lb_act = data['lb_act'].T

    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    np.random.seed(2121)
    shuffle_idx = np.random.permutation(lb_maxrel.shape[0])


    return ep[shuffle_idx], lb_maxrel[shuffle_idx], lb_pmb28[shuffle_idx], lb_pmb37[shuffle_idx], lb_act[shuffle_idx]



def load_data(location='dataset_original2.mat',is_total = False):
    # load eeg data
    data = sio.loadmat(location)
    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2], -1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb = data['lb'].T
    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    if is_total:
        np.random.seed(2121)
        shuffle_idx = np.random.permutation(lb.shape[0])
        return ep[shuffle_idx], lb[shuffle_idx]
    else:
        shuffle_idx = np.random.permutation(lb.shape[0])
        ep = ep[shuffle_idx]
        lb = lb[shuffle_idx]
        num_train = int(ep.shape[0] * 9 / 10)
        return ep[:num_train], lb[:num_train], ep[num_train:], lb[num_train:]


def _get_probs(y_train, x, nbin=1024):  # 50
    # _, edges = np.histogram(y_train, bins=self.nbin)
    # label_idx = np.digitize(y_train,edges)
    # try:
    #     label_idx[np.where(label_idx == (self.nbin+1))] = self.nbin
    # except:
    #     print('')
    # label_idx  = label_idx -1

    label_idx = y_train
    # make ont hot vector
    label_ = np.zeros((label_idx.shape[0], nbin))
    label_[range(label_idx.shape[0]), label_idx[:, 0]] = 1

    pys = np.sum(label_, axis=0) / float(label_.shape[0])

    u_a_x, u_indices_x, u_inverse_x, u_counts_x = np.unique(x, axis=0, return_index=True, return_inverse=True,
                                                            return_counts=True)
    u_a_x_ = x[u_indices_x]

    pxs = u_counts_x / float(np.sum(u_counts_x))

    p_y_given_x = []
    for i in range(0, len(u_a_x)):
        index = np.where(u_inverse_x == i)
        py_x = np.mean(label_[index, :], axis=0).reshape((-1,))
        p_y_given_x.append(py_x)

    u_a_y, u_indices_y, u_inverse_y, u_counts_y = np.unique(label_, axis=0, return_index=True, return_inverse=True,
                                                            return_counts=True)

    pys1 = u_counts_y / float(np.sum(u_counts_y))

    return pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs


def _get_information_for_layer(data, u_inverse_x, u_inverse_y, label, n_uax, pys, pxs, p_y_given_x, pys1):
    local_IXT, local_ITY = _get_sample_information(data, pys1, pxs, u_inverse_x, \
                                                   u_inverse_y)

    # information = {}
    # information['local_IXT'] = local_IXT
    # information['local_ITY'] = local_ITY
    information = [local_IXT, local_ITY]
    return information


def _get_sample_information(data, pys1, pxs, u_inverse_x, u_inverse_y):
    # edges = np.linspace(-1, 1, self.nbin+1) # to get truly 30 bins of data
    # data_digit = np.digitize(data,edges)
    # # change bin in max to be in max-1 bin
    # data_digit[np.where(data_digit == (self.nbin+1))] = self.nbin
    # data_digit = data_digit - 1

    unique_array, unique_inverse, unique_counts = np.unique(data, axis=0, return_index=False, return_inverse=True,
                                                            return_counts=True)
    p_ts = unique_counts / float(sum(unique_counts))
    PXs, PYs = np.asarray(pxs).T, np.asarray(pys1).T

    H2 = -np.sum(p_ts * np.log2(p_ts))
    H2X = _get_conditional_entropy(PXs, data, u_inverse_x)
    H2Y = _get_conditional_entropy(PYs.T, data, u_inverse_y)
    IY = H2 - H2Y
    IX = H2 - H2X
    return IX, IY


def _get_conditional_entropy(p, data, unique_inverse):
    # H2X_array = np.array([Parallel(n_jobs=NUM_CORES)(delayed(self._get_conditional_entropy_each)(data[unique_inverse == i, :], p[i])
    #                      for i in range(p.shape[0]))])
    H2X_array = np.array([_get_conditional_entropy_each(data[unique_inverse == i, :], p[i])
                          for i in range(p.shape[0])])
    H2X = np.sum(H2X_array)

    return H2X


def _get_conditional_entropy_each(data, p):
    _, _, unique_counts = np.unique(data, axis=0, return_index=False, return_inverse=True, return_counts=True)
    p_ts = unique_counts / float(sum(unique_counts))
    p_ts = np.asarray(p_ts, dtype=np.float64).T
    H2X = p * (-np.sum(p_ts * np.log2(p_ts)))
    return H2X


class networks():
    def __init__(self, ep):
        # build network
        self.lr = learning_rate = 0.001
        # 0.00003 was good
        ####### 아래 애
        self.c_lr = c_learning_rate = 3e-9  # 1e-8
        # training_epoch = 10
        self.training_epoch = training_epoch = 5000

        ##### 얘 조절하면 되는데 얘는 좀 고정하고 가자 위에 learning rate 조절로 쇼부보자
        self.c_training_epoch = c_training_epoch = 5000  # 2000
        # c_training_epoch = 2000 --> was good
        self.batch_size = batch_size = 22


        self.ep = ep
        self.num_input = self.ep.shape[1]

    def init_net(self):
        tf.reset_default_graph()

        self.X = tf.placeholder(tf.float32, [None, self.num_input])
        self.Y = tf.placeholder(tf.float32, [None, 1])

        # e_W0 = tf.get_variable("e_W0", shape=[self.num_input, 10240], initializer=tf.contrib.layers.xavier_initializer())
        # e_b0 = tf.get_variable("e_b0", shape=[10240], initializer=tf.contrib.layers.xavier_initializer())
        # e_W1 = tf.get_variable("e_W1", shape=[10240, 4961], initializer=tf.contrib.layers.xavier_initializer())
        e_W0 = tf.get_variable("e_W0", shape=[self.num_input, 4096], initializer=tf.keras.initializers.glorot_normal())
        e_b0 = tf.get_variable("e_b0", shape=[4096], initializer=tf.keras.initializers.glorot_normal())
        e_W1 = tf.get_variable("e_W1", shape=[4096, 2048], initializer=tf.keras.initializers.glorot_normal())
        e_b1 = tf.get_variable("e_b1", shape=[2048], initializer=tf.keras.initializers.glorot_normal())
        e_W2 = tf.get_variable("e_W2", shape=[2048, 1024], initializer=tf.keras.initializers.glorot_normal())
        e_b2 = tf.get_variable("e_b2", shape=[1024], initializer=tf.keras.initializers.glorot_normal())
        e_W3 = tf.get_variable("e_W3", shape=[1024, 512], initializer=tf.keras.initializers.glorot_normal())
        e_b3 = tf.get_variable("e_b3", shape=[512], initializer=tf.keras.initializers.glorot_normal())
        e_W4 = tf.get_variable("e_W4", shape=[512, 256], initializer=tf.keras.initializers.glorot_normal())
        e_b4 = tf.get_variable("e_b4", shape=[256], initializer=tf.keras.initializers.glorot_normal())
        e_W5 = tf.get_variable("e_W5", shape=[256, 128], initializer=tf.keras.initializers.glorot_normal())
        e_b5 = tf.get_variable("e_b5", shape=[128], initializer=tf.keras.initializers.glorot_normal())
        e_W6 = tf.get_variable("e_W6", shape=[128, 64], initializer=tf.keras.initializers.glorot_normal())
        e_b6 = tf.get_variable("e_b6", shape=[64], initializer=tf.keras.initializers.glorot_normal())
        e_W7 = tf.get_variable("e_W7", shape=[64, 32], initializer=tf.keras.initializers.glorot_normal())
        e_b7 = tf.get_variable("e_b7", shape=[32], initializer=tf.keras.initializers.glorot_normal())
        e_W8 = tf.get_variable("e_W8", shape=[32, 16], initializer=tf.keras.initializers.glorot_normal())
        e_b8 = tf.get_variable("e_b8", shape=[16], initializer=tf.keras.initializers.glorot_normal())
        e_W9 = tf.get_variable("e_W9", shape=[16, 8], initializer=tf.keras.initializers.glorot_normal())
        e_b9 = tf.get_variable("e_b9", shape=[8], initializer=tf.keras.initializers.glorot_normal())
        # self.encoder0 = tf.nn.tanh(tf.matmul(self.X, e_W0) + e_b0)
        # self.encoder1 = tf.nn.tanh(tf.matmul(self.encoder0, e_W1) + e_b1)

        self.encoder1 = tf.nn.tanh(tf.matmul(self.X, e_W0) + e_b0)
        self.encoder2 = tf.nn.tanh(tf.matmul(self.encoder1, e_W1) + e_b1)
        self.encoder3 = tf.nn.tanh(tf.matmul(self.encoder2, e_W2) + e_b2)
        self.encoder4 = tf.nn.tanh(tf.matmul(self.encoder3, e_W3) + e_b3)
        self.encoder5 = tf.nn.tanh(tf.matmul(self.encoder4, e_W4) + e_b4)
        self.encoder6 = tf.nn.tanh(tf.matmul(self.encoder5, e_W5) + e_b5)
        self.encoder7 = tf.nn.tanh(tf.matmul(self.encoder6, e_W6) + e_b6)
        self.encoder8 = tf.nn.tanh(tf.matmul(self.encoder7, e_W7) + e_b7)
        self.encoder9 = tf.nn.tanh(tf.matmul(self.encoder8, e_W8) + e_b8)
        self.encoder10 = tf.nn.tanh(tf.matmul(self.encoder9, e_W9) + e_b9)

        # d_W1 = tf.get_variable("d_W1", shape=[1024, 2048], initializer=tf.contrib.layers.xavier_initializer())
        # d_b1 = tf.get_variable("d_b1", shape=[2048], initializer=tf.contrib.layers.xavier_initializer())
        # d_W2 = tf.get_variable("d_W2", shape=[2048, 4096], initializer=tf.contrib.layers.xavier_initializer())
        # d_b2 = tf.get_variable("d_b2", shape=[4096], initializer=tf.contrib.layers.xavier_initializer())
        # d_W3 = tf.get_variable("d_W3", shape=[4096, num_input], initializer=tf.contrib.layers.xavier_initializer())
        # d_b3 = tf.get_variable("d_b3", shape=[num_input], initializer=tf.contrib.layers.xavier_initializer())

        # c_W = tf.get_variable("c_W", shape=[256, 128], initializer=tf.contrib.layers.xavier_initializer())
        # c_b = tf.get_variable("c_b", shape=[128], initializer=tf.contrib.layers.xavier_initializer())
        c_W2 = tf.get_variable("c_W2", shape=[8, 1], initializer=tf.keras.initializers.glorot_normal())
        c_b2 = tf.get_variable("c_b2", shape=[1], initializer=tf.keras.initializers.glorot_normal())
        # self.c_logit1 = tf.nn.tanh(tf.matmul(self.encoder5, c_W) + c_b)
        self.c_logit2 = tf.matmul(self.encoder10, c_W2) + c_b2
        self.c_sigmoid = tf.nn.sigmoid(self.c_logit2)
        self.c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y, logits=self.c_logit2))
        # tf.summary.scalar('loss', self.c_loss)

        # cost = tf.reduce_mean(tf.square(X - decoder))
        # optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

        self.c_optimizer = tf.train.AdamOptimizer(self.c_lr).minimize(self.c_loss)
        self.predicted = tf.cast(tf.nn.sigmoid(self.c_logit2) > 0.5, dtype=tf.float32)
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predicted, self.Y), dtype=tf.float32))
        # tf.summary.scalar('accuracy', self.accuracy)
        #
        # merged = tf.summary.merge_all()

        # train_summary_path, val_summary_path = "E:/dnn_example/logs6/train", "E:/dnn_example/logs6/val"
        # train_writer, val_writer = tf.summary.FileWriter(train_summary_path), tf.summary.FileWriter(val_summary_path)
        self.saver = tf.train.Saver(tf.global_variables())

    def restore_(self, ckptname='../eeg_pmb_overfit/model_100'):
        ckpt = tf.train.get_checkpoint_state(ckptname)
        with tf.Session() as sess:
            if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                self.saver.restore(sess, ckpt.model_checkpoint_path)
            else:
                sess.run(tf.global_variables_initializer())

    def return_hidden(self, CV_ori, epoch, dataset_idx, ep, lb, strings_):
        # kf = KFold(n_splits=10, shuffle=True)
        kf = KFold(n_splits=10, shuffle=False)
        np.random.seed(2020)
        index = np.random.permutation(ep.shape[0])
        ep = ep[index]
        lb = lb[index]

        kf.get_n_splits(lb)
        tenfold_hs = []
        accs = []
        losses = []
        CV = 0

        dataset_set = [0,1, 5]
        dataset = dataset_set[dataset_idx]

        summary_total = []
        lbs = []
        x=[]

        for train_ind, test_ind in kf.split(lb):
            for dataseti in [0, 1, 5]:
                summary_total.append({"hidden": [], "accuracy": [], "loss": [], "X": [], "labels": [], "epochs": []})
                #summary_total.append({"hidden": [], "accuracy": [], "loss": [], "epochs": []})

            ep_train, lb_train = ep[test_ind], lb[test_ind]
            batch_num = get_batch_num(ep_train, self.batch_size)
            feed_dict_train = {self.X: ep_train, self.Y: lb_train}
            encoder1_hidden, encoder2_hidden, encoder3_hidden, encoder4_hidden, encoder5_hidden, encoder6_hidden, \
            encoder7_hidden, encoder8_hidden, encoder9_hidden, encoder10_hidden, c_sigmoid_hidden, accuracy_hidden, c_loss_hidden  = sess.run(
                [self.encoder1, self.encoder2, self.encoder3, self.encoder4,
                 self.encoder5, self.encoder6, self.encoder7, self.encoder8,
                 self.encoder9, self.encoder10, self.c_sigmoid, self.accuracy*100, self.c_loss], feed_dict=feed_dict_train)
            #print("test") #encoder1_hidden len : 202, accuracy, loss = 1 value
            # import pdb;
            # pdb.set_trace()
            # pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs = _get_probs(
            #     np.array(lb_train).astype(np.int32), np.array(digitize_tolist_dist(ep_train)).astype(np.int32))

            hs = []
            hs.append(np.array(digitize_tolist_(encoder1_hidden)))
            hs.append(np.array(digitize_tolist_(encoder2_hidden)))
            hs.append(np.array(digitize_tolist_(encoder3_hidden)))
            hs.append(np.array(digitize_tolist_(encoder4_hidden)))
            hs.append(np.array(digitize_tolist_(encoder5_hidden)))
            hs.append(np.array(digitize_tolist_(encoder6_hidden)))
            hs.append(np.array(digitize_tolist_(encoder7_hidden)))
            hs.append(np.array(digitize_tolist_(encoder8_hidden)))
            hs.append(np.array(digitize_tolist_(encoder9_hidden)))
            hs.append(np.array(digitize_tolist_(encoder10_hidden)))
            hs.append(np.array(digitize_tolist(c_sigmoid_hidden)))

            x.append(np.array(ep_train))
            lbs.append(np.array(lb_train))

            accs.append(accuracy_hidden)
            losses.append(c_loss_hidden)
            tenfold_hs.append(hs)


            print(f'[ACCS] CV: {CV + 1} & accs = {accuracy_hidden}')
            print(f'[LOSS] CV: {CV + 1} & cost = {c_loss_hidden}')

            CV += 1

        summary_total[dataset_idx]["hidden"].append(tenfold_hs)
        summary_total[dataset_idx]["accuracy"].append(accs)
        summary_total[dataset_idx]["loss"].append(losses)
        summary_total[dataset_idx]["epochs"].append(epoch)
        # summary_total[dataset_idx]["labels"].append(lbs)
        # summary_total[dataset_idx]["X"].append(x)

        #pdb.set_trace()
        xandy = []
        xandy.append({"X": [], "Y": []})
        xandy[0]["X"].append(x)
        xandy[0]["Y"].append(lbs)

        direc = strings_ + '/logs6/part/cv{0}'.format(CV_ori) + f'/dataset{dataset}'
        #f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
        if not os.path.exists(direc):
            os.makedirs(direc)
        #pdb.set_trace()
        with open(direc + '/hidden'+str(epoch) + '.pkl', 'wb') as f: #open(f_name_dataset, 'wb') as f:
            pickle.dump(summary_total[dataset_idx], f)

        ## for making x, lbs
        direc2 = strings_ + '/logs6/part/cv{0}'.format(CV_ori) + f'/dataset{dataset}' +'/XandY'

        if not os.path.exists(direc2):
            os.makedirs(direc2)
        # pdb.set_trace()
        with open(direc2 + '/XandY' + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
            pickle.dump(xandy[0], f)

        """if not os.path.exists(direc2):
            os.makedirs(direc2)"""

        print('#' * 30)
        print(f'[ACCS] CV: AVG & accs = {np.mean(accs)}')
        print(f'[LOSS] CV: AVG & cost = {np.mean(losses)}')

        return accs, losses  # tenfold_infos, accs, losses

    def return_hidden_original(self, CV, epoch, dataset_idx_pr, ep, lb, strings_): #dataset_idx = 1 -> train, dataset_idx =2 -> test
        # kf = KFold(n_splits=10, shuffle=True)
        # kf.get_n_splits(lb)
        dataset_idx = dataset_idx_pr -1
        tenfold_hs = []
        accs = []
        losses = []

        dataset_set = ['train', 'test']
        dataset = dataset_set[dataset_idx]

        summary_total = []
        for dataseti in ['train', 'test']:
            #summary_total.append({"hidden": [], "accuracy": [], "loss": [], "X": [], "labels": [], "epochs": []})
            summary_total.append({"hidden": [], "accuracy": [], "loss": [], "epochs": []})

        # xandy = []
        # xandy.append({"X": [], "Y": []})

        ep_train, lb_train = ep, lb
        batch_num = get_batch_num(ep_train, self.batch_size)
        feed_dict_train = {self.X: ep_train, self.Y: lb_train}
        encoder1_hidden, encoder2_hidden, encoder3_hidden, encoder4_hidden, encoder5_hidden, encoder6_hidden, \
            encoder7_hidden, encoder8_hidden, encoder9_hidden, encoder10_hidden, c_sigmoid_hidden, accuracy_hidden, c_loss_hidden = sess.run(
            [self.encoder1, self.encoder2, self.encoder3, self.encoder4,
             self.encoder5, self.encoder6, self.encoder7, self.encoder8,
             self.encoder9, self.encoder10, self.c_sigmoid, self.accuracy * 100, self.c_loss], feed_dict=feed_dict_train)
        # print("test") #encoder1_hidden len : 202, accuracy, loss = 1 value
        # import pdb;
        # pdb.set_trace()
        # pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs = _get_probs(
        #     np.array(lb_train).astype(np.int32), np.array(digitize_tolist_dist(ep_train)).astype(np.int32))
        # lbs = []
        # x=[]
        hs = []

        hs.append(np.array(digitize_tolist_(encoder1_hidden)))
        hs.append(np.array(digitize_tolist_(encoder2_hidden)))
        hs.append(np.array(digitize_tolist_(encoder3_hidden)))
        hs.append(np.array(digitize_tolist_(encoder4_hidden)))
        hs.append(np.array(digitize_tolist_(encoder5_hidden)))
        hs.append(np.array(digitize_tolist_(encoder6_hidden)))
        hs.append(np.array(digitize_tolist_(encoder7_hidden)))
        hs.append(np.array(digitize_tolist_(encoder8_hidden)))
        hs.append(np.array(digitize_tolist_(encoder9_hidden)))
        hs.append(np.array(digitize_tolist_(encoder10_hidden)))
        hs.append(np.array(digitize_tolist(c_sigmoid_hidden)))

        accs.append(accuracy_hidden)
        losses.append(c_loss_hidden)
        tenfold_hs.append(hs)

        # lbs.append(lb_train)
        # x.append(np.array(digitize_tolist(ep_train)))


        # information = []

        # accs.append(sess.run(self.accuracy * 100, feed_dict={self.X: ep_train, self.Y: lb_train}))
        # print(f'[ACCS] CV: {CV + 1} & accs = {accs[-1]}')
        # losses.append(sess.run(self.c_loss, feed_dict={self.X: ep_train, self.Y: lb_train}))
        # print(f'[LOSS] CV: {CV + 1} & cost = {losses[-1]}')

        print(f'[ACCS] CV: {CV + 1} & accs = {accuracy_hidden}')
        print(f'[LOSS] CV: {CV + 1} & cost = {c_loss_hidden}')

        #
        # for i in range(len(hs)):
        #     data = hs[i]
        #     f_name_dataset = '/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
        #     with open(f_name_dataset, 'wb') as f:
        #         pickle.dump(hs, f)

        summary_total[dataset_idx]["hidden"].append(tenfold_hs)
        summary_total[dataset_idx]["accuracy"].append(accuracy_hidden)
        summary_total[dataset_idx]["loss"].append(c_loss_hidden)
        summary_total[dataset_idx]["epochs"].append(epoch)
        # summary_total[dataset_idx]["labels"].append(lbs)
        # summary_total[dataset_idx]["X"].append(x)

        x = np.array(ep_train)

        #pdb.set_trace()
        xandy = []
        xandy.append({"X": [], "Y": []})
        xandy[0]["X"].append(x)
        xandy[0]["Y"].append(lb_train)

        # try:
        #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
        # except:
        #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
        #         pickle.dump(summary_total[dataset_idx])

        # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'
        direc = strings_ + '/logs6/original/cv{0}'.format(CV) + f'/_{dataset}'
        # f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
        if not os.path.exists(direc):
            os.makedirs(direc)
        # pdb.set_trace()
        with open(direc + '/hidden' + str(epoch) + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
            pickle.dump(summary_total[dataset_idx], f)

        ## for making x, lbs
        direc2 = strings_ + '/logs6/original/cv{0}'.format(CV) + f'/_{dataset}' +'/XandY'
        # f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'

        if not os.path.exists(direc2):
            os.makedirs(direc2)
        # pdb.set_trace()
        with open(direc2 + '/XandY' + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
            pickle.dump(xandy[0], f)

        print('#' * 30)
        print(f'[ACCS] CV: AVG & accs = {np.mean(accs)}')
        print(f'[LOSS] CV: AVG & cost = {np.mean(losses)}')

        return accs, losses  # tenfold_infos, accs, losses


    def ext_hs(self, ep, lb, ckptname='E:/dnn_example/eeg_pmb_overfit/model_100', extra_dir='./'):
        ckpt = tf.train.get_checkpoint_state(ckptname)
        with tf.Session() as sess:
            if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                self.saver.restore(sess, ckpt.model_checkpoint_path)
            else:
                sess.run(tf.global_variables_initializer())
            # self.restore_(ckptname)
            batch_num = get_batch_num(ep, self.batch_size)
            feed_dict_train = {self.X: ep, self.Y: lb}

            # with AE
            # for epoch in range(training_epoch):
            #    total_cost = 0
            #    for i in range(batch_num):
            #        batch_ep = get_batch(ep, batch_size, i)
            #        _, batch_cost = sess.run([optimizer, cost], feed_dict={X: batch_ep})
            #        total_cost += batch_cost
            #    print(f'[AE] Epoch: {epoch+1} & Avg_cost = {total_cost/batch_num}')
            # print(f'Test Reconstruction Cost: {sess.run(cost, feed_dict={X: test_x})}')
            # print(f'Train Reconstruction Cost: {sess.run(cost, feed_dict={X: ep})}')

            epochs = []
            costs = []
            hidden_data = []
            summary = {}
            hidden_data_epoch = {}
            for epoch in range(self.c_training_epoch):
                if epoch % 10 == 0:
                    # hidden_data_epoch['~'].shape = (data_size, hidden_feature_size)
                    encoder1_hidden, encoder2_hidden, encoder3_hidden, encoder4_hidden, encoder5_hidden, c_sigmoid_hidden = sess.run(
                        [self.encoder1, self.encoder2,
                         self.encoder3, self.encoder4, self.encoder5, self.c_sigmoid], feed_dict=feed_dict_train)
                    hidden_data_epoch['epoch'] = epoch + 1
                    hidden_data_epoch['X'] = digitize_tolist_dist(ep)
                    hidden_data_epoch['Y'] = lb
                    hidden_data_epoch['encoder1'] = digitize_tolist_(encoder1_hidden, 1024)
                    hidden_data_epoch['encoder2'] = digitize_tolist_(encoder2_hidden, 1024)
                    hidden_data_epoch['encoder3'] = digitize_tolist_(encoder3_hidden, 1024)
                    hidden_data_epoch['encoder4'] = digitize_tolist_(encoder4_hidden, 1024)
                    hidden_data_epoch['encoder5'] = digitize_tolist_(encoder5_hidden, 1024)
                    hidden_data_epoch['c_sigmoid'] = digitize_tolist(c_sigmoid_hidden)
                    hidden_data.append(hidden_data_epoch)

                print(f'Train Accuracy: {sess.run(self.accuracy * 100, feed_dict={self.X: ep, self.Y: lb})}')
                summary["accuracy"] = sess.run(self.accuracy * 100, feed_dict={self.X: ep, self.Y: lb})
                epochs.append(epoch + 1)
                total_cost = 0
                for i in range(batch_num):
                    batch_ep = get_batch(ep, self.batch_size, i)
                    batch_lb = get_batch(lb, self.batch_size, i)
                    # _, batch_cost = sess.run([self.c_optimizer, self.c_loss], feed_dict={self.X: batch_ep, self.Y: batch_lb})
                    batch_cost = sess.run(self.c_loss, feed_dict={self.X: batch_ep, self.Y: batch_lb})
                    total_cost += batch_cost
                print(f'[Classifier] Epoch: {epoch + 1} & Avg_cost = {total_cost / batch_num}')
                costs.append(total_cost / batch_num)
                summary["loss"] = total_cost / batch_num

            if not os.path.exists(extra_dir):
                os.mkdir(extra_dir)
            with open(extra_dir + "/hidden.pkl", "wb") as f:
                pickle.dump(hidden_data_epoch, f)
            with open(extra_dir + "/summary.pkl", "wb") as f2:
                pickle.dump(summary, f2)
            # sio.savemat(extra_dir + "/hidden.mat", hidden_data_epoch)
            # sio.savemat(extra_dir + "/summary.mat", summary)

            return hidden_data_epoch, summary


##### main
print("main")
save_all = True

if save_all:
    eps = []
    lbs = []
    for dataset in [0]:
        # for dataset in [0]:
        if not os.path.exists(f'./dataset{dataset}'):
            os.mkdir(f'./dataset{dataset}')
        print(f'dataset{dataset}')
        ep_tot, lb_tot = load_data(f'dataset{dataset}_parsed.mat', is_total=True)
        eps.append(ep_tot)
        lbs.append(lb_tot)

print("loading...")
# if os.path.exists('dat_tot.pkl'):
#     with open('dat_tot.pkl','rb') as f:
#         data = pickle.load(f)
#         ep_tots = data['ep'].reshape((data['ep'].shape[0],-1))
#         lb_maxrel_tot = data['lb_maxrel']
#         lb_pmb28_tot = data['lb_pmb28']
#         lb_pmb37_tot = data['lb_pmb37']
#         lb_act_tot = data['lb_act']
# else:
#     ep_tots, lb_maxrel_tot, lb_pmb28_tot, lb_pmb37_tot, lb_act_tot = load_data_labels(
#         './dat_sub/dataset_total.mat')  # original2
# lb_tots = []
# lb_tots.append(lb_maxrel_tot)
# lb_tots.append(lb_pmb28_tot)
# lb_tots.append(lb_pmb37_tot)
# lb_tots.append(lb_act_tot)


ep_tots = []
lb_tots = []
strings_ = "./logs4_"+datetime.today().strftime('%Y%m%d-%H%M') +"/"
# for subi in [0,1,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,21,24,25,26,27,28,30]:
# for subi in range(33):
for subi in [0]:
    ep_tots_, lb_maxrel_tot, lb_pmb28_tot, lb_pmb37_tot, lb_act_tot = load_data_labels(
        './dat_sub/sub{0}.mat'.format(subi+1))  # original2
    ep_tots_ = (ep_tots_.reshape(ep_tots_.shape[0],16,-1))[:,:4,:]
    ep_tots_ = ep_tots_.reshape(ep_tots_.shape[0],-1)
    if len(ep_tots) == 0:
        ep_tots = ep_tots_
    else:
        ep_tots = np.concatenate((ep_tots, ep_tots_),axis=0)
    if len(lb_tots) == 0:
        lb_tots.append(lb_maxrel_tot)
        lb_tots.append(lb_pmb28_tot)
        lb_tots.append(lb_pmb37_tot)
        lb_tots.append(lb_act_tot)
    else:
        lb_tots[0]=np.concatenate((lb_tots[0],lb_maxrel_tot),axis=0)
        lb_tots[1]=np.concatenate((lb_tots[1],lb_pmb28_tot),axis=0)
        lb_tots[2]=np.concatenate((lb_tots[2],lb_pmb37_tot),axis=0)
        lb_tots[3]=np.concatenate((lb_tots[3],lb_act_tot),axis=0)



strings_="./logs20211018_subgroup_4ch/"
kf = KFold(n_splits=10, shuffle=False)
for lbi in [0]:
    lb_tot = lb_tots[lbi]
    lb1idx = np.where(lb_tot == 0)[0]
    lb2idx = np.where(lb_tot == 1)[0]
    minidx=min(lb1idx.shape[0],lb2idx.shape[0])
    lb1idx = lb1idx[:minidx]
    lb2idx = lb2idx[:minidx]
    lb_tot = np.concatenate((lb_tot[lb1idx],lb_tot[lb2idx]))
    ep_tot = np.concatenate((ep_tots[lb1idx,:],ep_tots[lb2idx,:]),axis=0)


    c = -1/(np.sqrt(2)*scipy.special.erfcinv(3/2))
    mad_ = c*np.median(np.abs(ep_tot-np.median(ep_tot,axis=1).reshape(-1,1)),axis=1)
    ep_tot = ep_tot[np.where(mad_<3)[0],:]
    lb_tot = lb_tot[np.where(mad_<3)[0]]


    np.random.seed(2020)
    index = np.random.permutation(ep_tot.shape[0])
    ep_tot = ep_tot[index, :]
    lb_tot = lb_tot[index]

    kf.get_n_splits(lb_tot)
    cv = 0

    print("main2")
    for train_ind, test_ind in kf.split(lb_tot):
        ep, lb = ep_tot[train_ind], lb_tot[train_ind]
        test_x, test_y = ep_tot[test_ind], lb_tot[test_ind]
        cv += 1

        if cv > 0:
            network = networks(ep_tot)
            network.init_net()
            # acc = []
            # loss = []
            infoxinfo = []
            # session = tf.Session(config=config....)    epoch = []
            # summary = {}
            # if not os.path.exists('./cv{0}'.format(cv)):
            #     os.mkdir('./cv{0}'.format(cv))
            config = tf.ConfigProto()
            config.gpu_options.allow_growth = True

            with tf.Session(config=config) as sess:
                saver = tf.train.Saver(tf.global_variables())
                tf.summary.scalar('loss', network.c_loss)
                tf.summary.scalar('accuracy', network.accuracy)
                merged = tf.summary.merge_all()
                train_summary_path, val_summary_path = strings_+ f'label{lbi+1}' + "/train",strings_+ f'label{lbi+1}' + "/test"
                train_writer, val_writer = tf.summary.FileWriter(train_summary_path), tf.summary.FileWriter(val_summary_path)

                # train autoencoder
                # ckpt = tf.train.get_checkpoint_state('./model')
                # if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                #  saver.restore(sess, ckpt.model_checkpoint_path)
                # else:
                #  sess.run(tf.global_variables_initializer())
                sess.run(tf.global_variables_initializer())
                batch_size = network.batch_size
                batch_num = get_batch_num(ep, batch_size)
                feed_dict_train = {network.X: ep, network.Y: lb.reshape(-1,1)}
                feed_dict_val = {network.X: test_x, network.Y: test_y.reshape(-1,1)}

                # summary_total = []
                # for dataset in [0, 5]:
                #     summary_total.append({"accuracy": [], "loss": [],
                #                           "epochs": []})  # "information": [], "accuracy": [], "loss": [], "epochs": []})
                epochs = []
                costs = []
                for epoch in tqdm(range(network.c_training_epoch)):
                    if save_all:
                        # orignal dataset 으로부터 뽑아내는 부분
                        if (epoch + 1) == 1:
                            print('#' * 30 + 'original dataset')
                            #train data
                            acc1, loss1 = network.return_hidden_original(cv, epoch, 1, ep, lb, strings_)
                            #test data
                            acc2, loss2 = network.return_hidden_original(cv, epoch, 2, test_x, test_y, strings_)

                            # original dataset 에 대해서 들어가야하고
                            # epoch 들어가야하고
                            # data 전부를 넣고 train test idx를 넣어야한다
                            # training data test data 구분해서 넣는다
                            # training test 다 들어가는데 어떻게하냐
                            # training test 둘다 뽑자.
                            # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'


                        # dataset 으로부터 뽑아내는 부분
                        if ((epoch + 1) == 1) and (cv == 1):
                            # for dataset_idx in range(3):
                            for dataset_idx in range(1):
                                dataset_set = [0,1,5]
                                dataset = dataset_set[dataset_idx]
                                print('#' * 30 + f'dataset{dataset}')
                                acc, loss = network.return_hidden(cv, epoch, dataset_idx, eps[dataset_idx], lbs[dataset_idx], strings_)

                            # summary_total[dataset_idx]["information"].append(info)
                            # summary_total[dataset_idx]["accuracy"].append(acc)
                            # summary_total[dataset_idx]["loss"].append(loss)
                            # summary_total[dataset_idx]["epochs"].append(epoch)
                            # try:
                            #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
                            # except:
                            #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
                            #         pickle.dump(summary_total[dataset_idx])

                    total_cost = 0
                    ####part
                    for i in range(batch_num):
                        batch_ep = get_batch(ep, batch_size, i)
                        batch_lb = get_batch(lb, batch_size, i)
                        _, batch_cost = sess.run([network.c_optimizer, network.c_loss],
                                                 feed_dict={network.X: batch_ep, network.Y: batch_lb.reshape(-1,1)})
                        total_cost += batch_cost
                    if (epoch + 1) % 10 == 0:
                        print(f'[Classifier] Epoch: {epoch + 1} & Avg_cost = {total_cost / batch_num}')
                    epochs.append(epoch + 1)
                    costs.append(total_cost / batch_num)

                    train_summary = sess.run(merged, feed_dict=feed_dict_train)
                    val_summary = sess.run(merged, feed_dict=feed_dict_val)

                    train_writer.add_summary(train_summary, global_step=epoch + 1)
                    val_writer.add_summary(val_summary, global_step=epoch + 1)

                    train_writer.flush()
                    val_writer.flush()
                    if (epoch + 1) % 100 == 0:
                        print(
                            f'Test Accuracy: {sess.run(network.accuracy * 100, feed_dict={network.X: test_x, network.Y: test_y.reshape(-1,1)})}')
                        print(f'Train Accuracy: {sess.run(network.accuracy * 100, feed_dict={network.X: ep, network.Y: lb.reshape(-1,1)})}')
                        cost_test = sess.run([network.c_loss], feed_dict={network.X: test_x, network.Y: test_y.reshape(-1,1)})
                        print(f'[Classifier] Epoch: {epoch + 1} & Test_cost = {cost_test}')

                    # if (epoch + 1) == 100:
                    #     if not os.path.exists('./cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1)):
                    #         os.makedirs('./cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1))
                    #     saver.save(sess, './cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1) + '/dnn.ckpt')
                    #
                    # if (epoch + 1) % 500 == 0:
                    #     if not os.path.exists('./cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1)):
                    #         os.makedirs('./cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1))
                    #     saver.save(sess, './cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1) + '/dnn.ckpt')

                    # orignal dataset 으로부터 뽑아내는 부분
                    if save_all:
                        if (epoch + 1) % 10 == 0:

                            # acc1, loss1 = network.return_hidden_original(epoch, eps[dataset_idx], lbs[dataset_idx])

                            # train data
                            acc1, loss1 = network.return_hidden_original(cv, epoch, 1, ep, lb, strings_)
                            # test data
                            acc2, loss2 = network.return_hidden_original(cv, epoch, 2, test_x, test_y, strings_)

                            # original dataset 에 대해서 들어가야하고
                            # epoch 들어가야하고
                            # data 전부를 넣고 train test idx를 넣어야한다
                            # training data test data 구분해서 넣는다
                            # training test 다 들어가는데 어떻게하냐
                            # training test 둘다 뽑자.
                            # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'

                        # dataset 으로부터 뽑아내는 부분
                        if ((epoch + 1) % 10 == 0) and (cv == 1):
                            # for dataset_idx in range(3):
                            for dataset_idx in range(1):
                                dataset_set = [0]
                                # dataset_set = [0,1,5]
                                dataset = dataset_set[dataset_idx]
                                print('#' * 30 + f'dataset{dataset}')
                                acc, loss = network.return_hidden(cv, epoch, dataset_idx, eps[dataset_idx], lbs[dataset_idx], strings_)
                                # dataset 받고, epoch '/cv{0}'.format(cv) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
                                # with open([파일이름],'wb') with f:
                                #   pickle.dump(f,[])
                                # summary_total[dataset_idx]["information"].append(info)
                                # summary_total[dataset_idx]["accuracy"].append(acc)
                                # summary_total[dataset_idx]["loss"].append(loss)
                                # summary_total[dataset_idx]["epochs"].append(epoch)
                                # try:
                                #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
                                # except:
                                #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
                                #         pickle.dump(summary_total[dataset_idx])

            try:
                del ep, test_x, lb, test_y
            except:
                ''

