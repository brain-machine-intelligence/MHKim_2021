# -*- coding: utf-8 -*-
"""classifier_zeroshot_tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15mDDi8VSjfED7GWZuIx8AVZQrWGR8OBd
"""
print("hi")
from scipy import signal
import scipy
import tensorflow.compat.v1 as tf
# import tensorflow as tf
tf.disable_v2_behavior()
import scipy.io as sio
import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import os
from tqdm import tqdm
import pickle
import pdb
import mat73
from datetime import datetime

#pdb.set_trace()

tf.device('/device:GPU:0')
tf.reset_default_graph()


# tf.debugging.set_log_device_placement(True)

def digitize_tolist(arr, bin_num=1024):  # 5
    if np.unique(arr).shape == (1,):
        arr = arr+np.random.rand(*arr.shape)*1e-6
    if arr.shape[0] == 1:
        return [[bin_num/2]]
    else:
        if np.any(np.isnan(arr)):
            arr[np.where(np.isnan(arr))] = np.nanmean(arr)
        if (np.min(arr) == 0) and (np.max(arr) == 0):
            return arr.astype(np.uint8).tolist()
        else:
            return np.digitize(arr, np.arange(np.min(arr), np.max(arr), (np.max(arr) - np.min(arr)) / bin_num)).astype(np.uint8).tolist()


def digitize_tolist_(arr, bin_num=1024):  # 5
    asdf = np.digitize(arr, np.arange(-1, 1, (1 - (-1)) / bin_num))
    return asdf.astype(np.uint8).tolist()


def digitize_tolist_dist(arr, bin_num=1024):  # 5
    bx = np.linspace(0, 100, bin_num + 1)
    bxx = []

    for bxi in bx:
        bxx.append(np.percentile(arr, bxi))
    bxx[-1] = bxx[-1] + 1
    bxx[0] = bxx[0] - 1
    return np.digitize(arr, bxx).astype(np.uint8).tolist()


def ext_spectrogram(epoch, fs=1000, window='hamming', nperseg=2000, noverlap=1975, nfft=3000):
    # epoch.shape = channel number, timepoint, trials
    # extract sepctrogram with time point

    dat = []
    for i in range(epoch.shape[2]):
        tfreq = []
        for j in range(epoch.shape[0]):
            f, t, Sxx = signal.stft(epoch[j, :, i], fs=fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft)
            interval = f[-1] / (len(f) - 1)
            req_len = int(40 / interval)
            # use frequency(~121th) and tiem(-41th~0)
            tfreq.append(np.abs(Sxx[:121, -41:]).transpose())
            # tfreq.append(np.abs(Sxx[:121, :]).transpose())

        dat.append(np.asarray(tfreq))

    return np.array(dat)  # shape : (trials, channel number, time, freq), time and freq should be : 41, 121


def get_batch_num(data, batch_size):
    total_len = data.shape[0]
    return math.ceil(total_len / batch_size)


def get_batch(data, batch_size, idx):
    batch_num = get_batch_num(data, batch_size)
    if idx == batch_num - 1:  # last batch
        return data[batch_size * idx:]
    else:
        return data[batch_size * idx:batch_size * (idx + 1)]


# input: location
# output: train_x, train_y, test_x, test_y

def load_data_labels(location='dataset_original2.mat'):
    # load eeg data
    try:
        data = sio.loadmat(location)
    except:
        data = mat73.loadmat(location)

    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2],-1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb_maxrel = data['lb_maxrel'].T
    lb_pmb28 = data['lb_pmb28'].T
    lb_pmb37 = data['lb_pmb37'].T
    lb_act = data['lb_act'].T

    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    np.random.seed(2121)
    shuffle_idx = np.random.permutation(lb_maxrel.shape[0])


    return ep[shuffle_idx], lb_maxrel[shuffle_idx], lb_pmb28[shuffle_idx], lb_pmb37[shuffle_idx], lb_act[shuffle_idx]



def load_data(location='dataset_original2.mat',is_total = False):
    # load eeg data
    data = sio.loadmat(location)
    # get eeg data and extract spectogram
    # reshpae spectogram data as shape (num_trials, features) to use it in FC
    ep = ext_spectrogram(data['ep']).reshape(data['ep'].shape[2], -1)
    # get label data
    # reshape it to (num_trials, 1) to use it in FC
    lb = data['lb'].T
    # check shape of ep & lb
    # print(ep.shape, lb.shape)
    # generate random index, for unbiased dataset
    # shuffle_idx = np.arange(ep.shape[0])
    # np.random.shuffle(shuffle_idx)
    # shuffle ep and lb in the same order
    if is_total:
        np.random.seed(2121)
        shuffle_idx = np.random.permutation(lb.shape[0])
        return ep[shuffle_idx], lb[shuffle_idx]
    else:
        shuffle_idx = np.random.permutation(lb.shape[0])
        ep = ep[shuffle_idx]
        lb = lb[shuffle_idx]
        num_train = int(ep.shape[0] * 9 / 10)
        return ep[:num_train], lb[:num_train], ep[num_train:], lb[num_train:]


def _get_probs(y_train, x, nbin=1024):  # 50
    # _, edges = np.histogram(y_train, bins=self.nbin)
    # label_idx = np.digitize(y_train,edges)
    # try:
    #     label_idx[np.where(label_idx == (self.nbin+1))] = self.nbin
    # except:
    #     print('')
    # label_idx  = label_idx -1

    label_idx = y_train
    # make ont hot vector
    label_ = np.zeros((label_idx.shape[0], nbin))
    label_[range(label_idx.shape[0]), label_idx[:, 0]] = 1

    pys = np.sum(label_, axis=0) / float(label_.shape[0])

    u_a_x, u_indices_x, u_inverse_x, u_counts_x = np.unique(x, axis=0, return_index=True, return_inverse=True,
                                                            return_counts=True)
    u_a_x_ = x[u_indices_x]

    pxs = u_counts_x / float(np.sum(u_counts_x))

    p_y_given_x = []
    for i in range(0, len(u_a_x)):
        index = np.where(u_inverse_x == i)
        py_x = np.mean(label_[index, :], axis=0).reshape((-1,))
        p_y_given_x.append(py_x)

    u_a_y, u_indices_y, u_inverse_y, u_counts_y = np.unique(label_, axis=0, return_index=True, return_inverse=True,
                                                            return_counts=True)

    pys1 = u_counts_y / float(np.sum(u_counts_y))

    return pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs


def _get_information_for_layer(data, u_inverse_x, u_inverse_y, label, n_uax, pys, pxs, p_y_given_x, pys1):
    local_IXT, local_ITY = _get_sample_information(data, pys1, pxs, u_inverse_x, \
                                                   u_inverse_y)

    # information = {}
    # information['local_IXT'] = local_IXT
    # information['local_ITY'] = local_ITY
    information = [local_IXT, local_ITY]
    return information


def _get_sample_information(data, pys1, pxs, u_inverse_x, u_inverse_y):
    # edges = np.linspace(-1, 1, self.nbin+1) # to get truly 30 bins of data
    # data_digit = np.digitize(data,edges)
    # # change bin in max to be in max-1 bin
    # data_digit[np.where(data_digit == (self.nbin+1))] = self.nbin
    # data_digit = data_digit - 1

    unique_array, unique_inverse, unique_counts = np.unique(data, axis=0, return_index=False, return_inverse=True,
                                                            return_counts=True)
    p_ts = unique_counts / float(sum(unique_counts))
    PXs, PYs = np.asarray(pxs).T, np.asarray(pys1).T

    H2 = -np.sum(p_ts * np.log2(p_ts))
    H2X = _get_conditional_entropy(PXs, data, u_inverse_x)
    H2Y = _get_conditional_entropy(PYs.T, data, u_inverse_y)
    IY = H2 - H2Y
    IX = H2 - H2X
    return IX, IY


def _get_conditional_entropy(p, data, unique_inverse):
    # H2X_array = np.array([Parallel(n_jobs=NUM_CORES)(delayed(self._get_conditional_entropy_each)(data[unique_inverse == i, :], p[i])
    #                      for i in range(p.shape[0]))])
    H2X_array = np.array([_get_conditional_entropy_each(data[unique_inverse == i, :], p[i])
                          for i in range(p.shape[0])])
    H2X = np.sum(H2X_array)

    return H2X


def _get_conditional_entropy_each(data, p):
    _, _, unique_counts = np.unique(data, axis=0, return_index=False, return_inverse=True, return_counts=True)
    p_ts = unique_counts / float(sum(unique_counts))
    p_ts = np.asarray(p_ts, dtype=np.float64).T
    H2X = p * (-np.sum(p_ts * np.log2(p_ts)))
    return H2X


class networks():
    def __init__(self, ep):
        # build network
        self.lr = learning_rate = 0.001
        # 0.00003 was good
        ####### 아래 애
        self.c_lr = c_learning_rate = 7e-6  # 1e-8
        # training_epoch = 10
        self.training_epoch = training_epoch = 3000

        ##### 얘 조절하면 되는데 얘는 좀 고정하고 가자 위에 learning rate 조절로 쇼부보자
        self.c_training_epoch = c_training_epoch = 5000  # 2000
        # c_training_epoch = 2000 --> was good
        self.batch_size = batch_size =20

        self.ep = ep
        self.num_input = self.ep.shape[1]

    def init_net(self):
        tf.reset_default_graph()

        # self.X = tf.placeholder(tf.float32, [None, self.num_input])
        self.X = tf.placeholder(tf.float32, [None, 121*2, 41*2, 1])
        self.Y = tf.placeholder(tf.float32, [None, 1])
        # X = tf.placeholder(tf.float32, [None, 121*4, 41*4, 1])
        # Y = tf.placeholder(tf.float32, [None, 1])

        # bat_normzer = tf.layers.BatchNormalization()
        # self.norXm = bat_normzer(self.X)

        # w1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 16), stddev=0.1))
        # w1 = tf.get_variable("e_W1",shape=[7, 7, 1, 1], initializer=tf.contrib.layers.xavier_initializer())
        w1 = tf.get_variable("e_W1",shape=[7, 7, 1, 16], initializer=tf.keras.initializers.glorot_normal())
        # w1 = tf.get_variable("e_W1",shape=[7, 7, 1, 4], initializer=tf.contrib.layers.xavier_initializer())
        conv1 = tf.nn.conv2d(self.X, w1, strides=[1, 2, 2, 1], padding="VALID")
        self.conv1_out = tf.nn.tanh(conv1)
        self.pool1 = tf.nn.max_pool(self.conv1_out, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')

        # w2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 32), stddev=0.1))
        # w2 = tf.get_variable("e_W2",shape=[7, 7,  1, 1], initializer=tf.contrib.layers.xavier_initializer())
        w2 = tf.get_variable("e_W2",shape=[5, 5, 16, 64], initializer=tf.keras.initializers.glorot_normal())
        # w2 = tf.get_variable("e_W2",shape=[5, 5, 4, 8], initializer=tf.contrib.layers.xavier_initializer())
        conv2 = tf.nn.conv2d(self.pool1, w2, strides=[1, 2, 2, 1], padding="VALID")
        self.conv2_out = tf.nn.tanh(conv2)
        self.pool2 = tf.nn.max_pool(self.conv2_out, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')

        # w3 = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 64), stddev=0.1))
        # w3 = tf.get_variable("e_W3",shape=[7, 7, 32, 64], initializer=tf.contrib.layers.xavier_initializer())
        w3 = tf.get_variable("e_W3",shape=[5, 5, 64, 128], initializer=tf.keras.initializers.glorot_normal())
        # w3 = tf.get_variable("e_W3",shape=[5, 5, 8, 32], initializer=tf.contrib.layers.xavier_initializer())
        conv3 = tf.nn.conv2d(self.pool2, w3, strides=[1, 2, 2, 1], padding="VALID")
        self.conv3_out = tf.nn.tanh(conv3)
        self.pool3 = tf.nn.max_pool(self.conv3_out, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')

        # gap = tf.reduce_mean(tf.reshape(pool1, shape=(-1,80*27,1)), axis=1)
        self.gap = tf.reduce_mean(tf.reshape(self.pool3, shape=(-1,27*7,128)), axis=1)
        # gap = tf.reduce_mean(tf.reshape(pool3, shape=(-1,27*7,32)), axis=1)
        # gap = tf.reduce_mean(tf.reshape(pool2, shape=(-1,58*18,32)), axis=1)

        # self.weight_ = tf.Variable(tf.zeros((1, 1), dtype=tf.float32), dtype=tf.float32)
        # self.weight_ = tf.Variable(tf.zeros((256, 1), dtype=tf.float32), dtype=tf.float32)
        # self.weight_ = tf.get_variable("weight_",shape=[32,1], initializer=tf.contrib.layers.xavier_initializer())
        self.weight_ = tf.get_variable("weight_",shape=[128,1], initializer=tf.keras.initializers.glorot_normal())
        # self.weight_ = tf.Variable(tf.zeros((32, 1), dtype=tf.float32), dtype=tf.float32)
        # self.bias_ = tf.Variable(tf.zeros((1), dtype=tf.float32), dtype=tf.float32)
        self.bias_ = tf.get_variable("bias_",shape=[1],initializer=tf.keras.initializers.glorot_normal())
        self.c_logit2 = tf.matmul(self.gap, self.weight_) + self.bias_

        self.c_sigmoid = tf.nn.sigmoid(self.c_logit2)
        self.c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y, logits=self.c_logit2))


        self.c_optimizer = tf.train.AdamOptimizer(self.c_lr).minimize(self.c_loss)
        self.predicted = tf.cast(tf.nn.sigmoid(self.c_logit2) > 0.5, dtype=tf.float32)
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predicted, self.Y), dtype=tf.float32))

        # train_summary_path, val_summary_path = "E:/dnn_example/logs6/train", "E:/dnn_example/logs6/val"
        # train_writer, val_writer = tf.summary.FileWriter(train_summary_path), tf.summary.FileWriter(val_summary_path)
        self.saver = tf.train.Saver(tf.global_variables())

    def restore_(self, ckptname='../eeg_pmb_overfit/model_100'):
        ckpt = tf.train.get_checkpoint_state(ckptname)
        with tf.Session() as sess:
            if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                self.saver.restore(sess, ckpt.model_checkpoint_path)
            else:
                sess.run(tf.global_variables_initializer())

    def return_hidden(self, CV_ori, epoch, dataset_idx, ep, lb, strings_):
        # kf = KFold(n_splits=10, shuffle=True)
        kf = KFold(n_splits=10, shuffle=False)
        np.random.seed(2020)
        index = np.random.permutation(ep.shape[0])
        ep = ep[index, :]
        lb = lb[index]

        kf.get_n_splits(lb)
        tenfold_hs = []
        accs = []
        losses = []
        CV = 0

        dataset_set = [0, 1, 5]
        dataset = dataset_set[dataset_idx]

        lbs = []
        x=[]

        for train_ind, test_ind in kf.split(lb):
            summary_total = []
            # for dataseti in [0, 1, 5]:
            summary_total.append({"hidden": [], "accuracy": [], "loss": [], "X": [], "labels": [], "epochs": []})
                #summary_total.append({"hidden": [], "accuracy": [], "loss": [], "epochs": []})
            ep_train, lb_train = ep[test_ind], lb[test_ind]
            ep_train = ep_train.reshape(-1,4,121,41)
            ep_train = np.concatenate((np.concatenate((ep_train[:,0,:,:], ep_train[:,1,:,:]),axis=1),np.concatenate((ep_train[:,2,:,:], ep_train[:,3,:,:]),axis=1)),axis=2).reshape(-1,242,82,1)
            batch_num = get_batch_num(ep_train, self.batch_size)
            feed_dict_train = {self.X: ep_train, self.Y: lb_train}
            conv1_out, pool1, conv2_out, pool2, conv3_out, pool3, gap, c_logit2, c_sigmoid, accuracy_hidden, c_loss_hidden = sess.run(
                [self.conv1_out, self.pool1, self.conv2_out, self.pool2,
                 self.conv3_out, self.pool3, self.gap, self.c_logit2, self.c_sigmoid, self.accuracy*100, self.c_loss], feed_dict=feed_dict_train)
            #print("test") #encoder1_hidden len : 202, accuracy, loss = 1 value
            # import pdb;
            # pdb.set_trace()
            # pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs = _get_probs(
            #     np.array(lb_train).astype(np.int32), np.array(digitize_tolist_dist(ep_train)).astype(np.int32))

            hs = []
            conv1_out_ = []
            conv2_out_ = []
            conv3_out_ = []
            pool1_ = []
            pool2_ = []
            pool3_ = []
            gap_ = []
            c_logit2_ = []
            c_sigmoid_ = []
            for cic in range(11):
                magic1 = round(len(conv1_out) / 11)
                if len(conv1_out_) == 0:
                    conv1_out_ = np.array(digitize_tolist_(conv1_out[cic * magic1:(cic + 1) * magic1]))
                else:
                    conv1_out_ = np.concatenate((conv1_out_, np.array(digitize_tolist_(conv1_out[cic * magic1:(cic + 1) * magic1]))),
                                   axis=0)

                magic1 = round(len(conv2_out) / 11)
                if len(conv2_out_) == 0:
                    conv2_out_ = np.array(digitize_tolist_(conv2_out[cic * magic1:(cic + 1) * magic1]))
                else:
                    conv2_out_ = np.concatenate((conv2_out_, np.array(digitize_tolist_(conv2_out[cic * magic1:(cic + 1) * magic1]))),
                                   axis=0)

                magic1 = round(len(conv3_out) / 11)
                if len(conv3_out_) == 0:
                    conv3_out_ = np.array(digitize_tolist_(conv3_out[cic * magic1:(cic + 1) * magic1]))
                else:
                    conv3_out_ = np.concatenate((conv3_out_, np.array(digitize_tolist_(conv3_out[cic * magic1:(cic + 1) * magic1]))),
                                   axis=0)

                magic1 = round(len(pool1) / 11)
                if len(pool1_) == 0:
                    pool1_ = np.array(digitize_tolist_(pool1[cic * magic1:(cic + 1) * magic1]))
                else:
                    pool1_ = np.concatenate((pool1_, np.array(digitize_tolist_(pool1[cic * magic1:(cic + 1) * magic1]))), axis=0)

                magic1 = round(len(pool2) / 11)
                if len(pool2_) == 0:
                    pool2_ = np.array(digitize_tolist_(pool2[cic * magic1:(cic + 1) * magic1]))
                else:
                    pool2_ = np.concatenate((pool2_, np.array(digitize_tolist_(pool2[cic * magic1:(cic + 1) * magic1]))), axis=0)

                magic1 = round(len(pool3) / 11)
                if len(pool3_) == 0:
                    pool3_ = np.array(digitize_tolist_(pool3[cic * magic1:(cic + 1) * magic1]))
                else:
                    pool3_ = np.concatenate((pool3_, np.array(digitize_tolist_(pool3[cic * magic1:(cic + 1) * magic1]))), axis=0)

                magic1 = round(len(gap) / 11)
                if len(gap_) == 0:
                    gap_ = np.array(digitize_tolist_(gap[cic * magic1:(cic + 1) * magic1]))
                else:
                    gap_ = np.concatenate((gap_, np.array(digitize_tolist_(gap[cic * magic1:(cic + 1) * magic1]))), axis=0)

                magic1 = round(len(c_logit2) / 11)
                if len(c_logit2_) == 0:
                    c_logit2_ = np.array(digitize_tolist(c_logit2[cic * magic1:(cic + 1) * magic1]))
                else:
                    c_logit2_ = np.concatenate((c_logit2_, np.array(digitize_tolist(c_logit2[cic * magic1:(cic + 1) * magic1]))),
                                   axis=0)

                magic1 = round(len(c_sigmoid) / 11)
                if len(c_sigmoid_) == 0:
                    c_sigmoid_ = np.array(digitize_tolist(c_sigmoid[cic * magic1:(cic + 1) * magic1]))
                else:
                    c_sigmoid_ = np.concatenate((c_sigmoid_, np.array(digitize_tolist(c_sigmoid[cic * magic1:(cic + 1) * magic1]))),
                                   axis=0)

            hs.append(conv1_out_);
            del conv1_out
            hs.append(pool1_);
            del pool1
            hs.append(conv2_out_);
            del conv2_out
            hs.append(pool2_);
            del pool2
            hs.append(conv3_out_);
            del conv3_out
            hs.append(pool3_);
            del pool3
            hs.append(gap_);
            del gap
            hs.append(c_logit2_);
            del c_logit2
            hs.append(c_sigmoid_);
            del c_sigmoid

            # x.append(ep_train)
            # lbs.append(lb_train)

            # accs.append(accuracy_hidden)
            # losses.append(c_loss_hidden)
            # tenfold_hs.append(hs)


            print(f'[ACCS] CV: {CV + 1} & accs = {accuracy_hidden}')
            print(f'[LOSS] CV: {CV + 1} & cost = {c_loss_hidden}')

            CV += 1

            summary_total[0]["hidden"].append(hs)
            summary_total[0]["accuracy"].append(accuracy_hidden)
            summary_total[0]["loss"].append(c_loss_hidden)
            summary_total[0]["epochs"].append(epoch)
            # summary_total[dataset_idx]["labels"].append(lbs)
            # summary_total[dataset_idx]["X"].append(x)

            #pdb.set_trace()
            xandy = []
            xandy.append({"X": [], "Y": []})
            xandy[0]["X"].append(ep_train)
            xandy[0]["Y"].append(lb_train)

            direc = strings_ + '/logs6/part/cv{0}'.format(CV_ori) + f'/dataset{dataset}' + '/cv{0}'.format(CV)
            #f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
            if not os.path.exists(direc):
                os.makedirs(direc)
            #pdb.set_trace()
            with open(direc + '/hidden'+str(epoch) + '.pkl', 'wb') as f: #open(f_name_dataset, 'wb') as f:
                pickle.dump(summary_total[0], f)

            ## for making x, lbs
            direc2 = strings_ + '/logs6/part/cv{0}'.format(CV_ori) + f'/dataset{dataset}'+ '/cv{0}'.format(CV) +'/XandY'

            if not os.path.exists(direc2):
                os.makedirs(direc2)
            # pdb.set_trace()
            with open(direc2 + '/XandY' + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
                pickle.dump(xandy[0], f)

            """if not os.path.exists(direc2):
                os.makedirs(direc2)"""

            print('#' * 30)
            print(f'[ACCS] CV: AVG & accs = {np.mean(accs)}')
            print(f'[LOSS] CV: AVG & cost = {np.mean(losses)}')

        return accs, losses  # tenfold_infos, accs, losses

    def return_hidden_original(self, CV, epoch, dataset_idx_pr, ep, lb, strings_): #dataset_idx = 1 -> train, dataset_idx =2 -> test
        # kf = KFold(n_splits=10, shuffle=True)
        # kf.get_n_splits(lb)
        dataset_idx = dataset_idx_pr -1
        tenfold_hs = []
        accs = []
        losses = []

        dataset_set = ['train', 'test']
        dataset = dataset_set[dataset_idx]

        summary_total = []
        for dataseti in ['train', 'test']:
            #summary_total.append({"hidden": [], "accuracy": [], "loss": [], "X": [], "labels": [], "epochs": []})
            summary_total.append({"hidden": [], "accuracy": [], "loss": [], "epochs": []})

        # xandy = []
        # xandy.append({"X": [], "Y": []})

        ep_train, lb_train = ep, lb
        batch_num = get_batch_num(ep_train, self.batch_size)
        feed_dict_train = {self.X: ep_train, self.Y: lb_train}
        conv1_out, pool1, conv2_out, pool2, conv3_out, pool3, gap, c_logit2, c_sigmoid, accuracy_hidden, c_loss_hidden = sess.run(
            [self.conv1_out, self.pool1, self.conv2_out, self.pool2,
             self.conv3_out, self.pool3, self.gap, self.c_logit2, self.c_sigmoid, self.accuracy*100, self.c_loss], feed_dict=feed_dict_train)
        # print("test") #encoder1_hidden len : 202, accuracy, loss = 1 value
        # import pdb;
        # pdb.set_trace()
        # pys, pys1, p_y_given_x, u_a_x_, u_inverse_x, u_inverse_y, pxs = _get_probs(
        #     np.array(lb_train).astype(np.int32), np.array(digitize_tolist_dist(ep_train)).astype(np.int32))
        # lbs = []
        # x=[]
        if np.any(np.isnan(c_logit2)):
            print('asd')
        if np.any(np.isnan(c_loss_hidden)):
            print('asd')

        hs = []
        conv1_out_=[]
        conv2_out_=[]
        conv3_out_=[]
        pool1_=[]
        pool2_=[]
        pool3_=[]
        gap_=[]
        c_logit2_=[]
        c_sigmoid_=[]

        # for cic in range(11):
        #     magic1 = round(len(conv1_out) / 11)
        #     if len(conv1_out_) == 0:
        #         conv1_out_ = np.array(digitize_tolist_(conv1_out[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         conv1_out_ = np.concatenate((conv1_out_, np.array(digitize_tolist_(conv1_out[cic * magic1:(cic + 1) * magic1]))),
        #                        axis=0)
        #
        #     magic1 = round(len(conv2_out) / 11)
        #     if len(conv2_out_) == 0:
        #         conv2_out_ = np.array(digitize_tolist_(conv2_out[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         conv2_out_ = np.concatenate((conv2_out_, np.array(digitize_tolist_(conv2_out[cic * magic1:(cic + 1) * magic1]))),
        #                        axis=0)
        #
        #     magic1 = round(len(conv3_out) / 11)
        #     if len(conv3_out_) == 0:
        #         conv3_out_ = np.array(digitize_tolist_(conv3_out[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         conv3_out_ = np.concatenate((conv3_out_, np.array(digitize_tolist_(conv3_out[cic * magic1:(cic + 1) * magic1]))),
        #                        axis=0)
        #
        #     magic1 = round(len(pool1) / 11)
        #     if len(pool1_) == 0:
        #         pool1_ = np.array(digitize_tolist_(pool1[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         pool1_ = np.concatenate((pool1_, np.array(digitize_tolist_(pool1[cic * magic1:(cic + 1) * magic1]))), axis=0)
        #
        #     magic1 = round(len(pool2) / 11)
        #     if len(pool2_) == 0:
        #         pool2_ = np.array(digitize_tolist_(pool2[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         pool2_ = np.concatenate((pool2_, np.array(digitize_tolist_(pool2[cic * magic1:(cic + 1) * magic1]))), axis=0)
        #
        #     magic1 = round(len(pool3) / 11)
        #     if len(pool3_) == 0:
        #         pool3_ = np.array(digitize_tolist_(pool3[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         pool3_ = np.concatenate((pool3_, np.array(digitize_tolist_(pool3[cic * magic1:(cic + 1) * magic1]))), axis=0)
        #
        #     magic1 = round(len(gap) / 11)
        #     if len(gap_) == 0:
        #         gap_ = np.array(digitize_tolist_(gap[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         gap_ = np.concatenate((gap_, np.array(digitize_tolist_(gap[cic * magic1:(cic + 1) * magic1]))), axis=0)
        #
        #     if np.any(np.isnan(c_logit2)):
        #         print('s')
        #     magic1 = round(len(c_logit2) / 11)
        #     if len(c_logit2_) == 0:
        #         c_logit2_ = np.array(digitize_tolist(c_logit2[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         c_logit2_ = np.concatenate((c_logit2_, np.array(digitize_tolist(c_logit2[cic * magic1:(cic + 1) * magic1]))),
        #                        axis=0)
        #
        #     magic1 = round(len(c_sigmoid) / 11)
        #     if len(c_sigmoid_) == 0:
        #         c_sigmoid_ = np.array(digitize_tolist(c_sigmoid[cic * magic1:(cic + 1) * magic1]))
        #     else:
        #         c_sigmoid_ = np.concatenate((c_sigmoid_, np.array(digitize_tolist(c_sigmoid[cic * magic1:(cic + 1) * magic1]))),
        #                        axis=0)
        # hs.append(conv1_out_); del conv1_out
        # hs.append(pool1_); del pool1
        # hs.append(conv2_out_); del conv2_out
        # hs.append(pool2_); del pool2
        # hs.append(conv3_out_); del conv3_out
        # hs.append(pool3_); del pool3
        # hs.append(gap_); del gap
        # hs.append(c_logit2_); del c_logit2
        # hs.append(c_sigmoid_); del c_sigmoid


        hs.append(digitize_tolist_(conv1_out)); del conv1_out
        hs.append(digitize_tolist_(pool1)); del pool1
        hs.append(digitize_tolist_(conv2_out)); del conv2_out
        hs.append(digitize_tolist_(pool2)); del pool2
        hs.append(digitize_tolist_(conv3_out)); del conv3_out
        hs.append(digitize_tolist_(pool3)); del pool3
        hs.append(digitize_tolist_(gap)); del gap
        hs.append(digitize_tolist(c_logit2)); del c_logit2
        hs.append(digitize_tolist(c_sigmoid)); del c_sigmoid


        accs.append(accuracy_hidden)
        losses.append(c_loss_hidden)
        tenfold_hs.append(hs)

        # lbs.append(lb_train)
        # x.append(np.array(digitize_tolist(ep_train)))


        # information = []

        # accs.append(sess.run(self.accuracy * 100, feed_dict={self.X: ep_train, self.Y: lb_train}))
        # print(f'[ACCS] CV: {CV + 1} & accs = {accs[-1]}')
        # losses.append(sess.run(self.c_loss, feed_dict={self.X: ep_train, self.Y: lb_train}))
        # print(f'[LOSS] CV: {CV + 1} & cost = {losses[-1]}')

        print(f'[ACCS] CV: {CV + 1} & accs = {accuracy_hidden}')
        print(f'[LOSS] CV: {CV + 1} & cost = {c_loss_hidden}')

        #
        # for i in range(len(hs)):
        #     data = hs[i]
        #     f_name_dataset = '/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
        #     with open(f_name_dataset, 'wb') as f:
        #         pickle.dump(hs, f)

        summary_total[dataset_idx]["hidden"].append(tenfold_hs)
        summary_total[dataset_idx]["accuracy"].append(accuracy_hidden)
        summary_total[dataset_idx]["loss"].append(c_loss_hidden)
        summary_total[dataset_idx]["epochs"].append(epoch)
        # summary_total[dataset_idx]["labels"].append(lbs)
        # summary_total[dataset_idx]["X"].append(x)

        x = np.array(ep_train)

        #pdb.set_trace()
        xandy = []
        xandy.append({"X": [], "Y": []})
        xandy[0]["X"].append(x)
        xandy[0]["Y"].append(lb_train)

        # try:
        #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
        # except:
        #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
        #         pickle.dump(summary_total[dataset_idx])

        # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'
        direc = strings_ + '/logs6/original/cv{0}'.format(CV) + f'/_{dataset}'
        # f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
        if not os.path.exists(direc):
            os.makedirs(direc)
        # pdb.set_trace()
        with open(direc + '/hidden' + str(epoch) + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
            pickle.dump(summary_total[dataset_idx], f)

        ## for making x, lbs
        direc2 = strings_ + '/logs6/original/cv{0}'.format(CV) + f'/_{dataset}' +'/XandY'
        # f_name_dataset = 'E:/dnn_example/logs/cv{0}'.format(CV) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'

        if not os.path.exists(direc2):
            os.makedirs(direc2)
        # pdb.set_trace()
        with open(direc2 + '/XandY' + '.pkl', 'wb') as f:  # open(f_name_dataset, 'wb') as f:
            pickle.dump(xandy[0], f)

        print('#' * 30)
        print(f'[ACCS] CV: AVG & accs = {np.mean(accs)}')
        print(f'[LOSS] CV: AVG & cost = {np.mean(losses)}')

        return accs, losses  # tenfold_infos, accs, losses


    def ext_hs(self, ep, lb, ckptname='E:/dnn_example/eeg_pmb_overfit/model_100', extra_dir='./'):
        ckpt = tf.train.get_checkpoint_state(ckptname)
        with tf.Session() as sess:
            if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                self.saver.restore(sess, ckpt.model_checkpoint_path)
            else:
                sess.run(tf.global_variables_initializer())
            # self.restore_(ckptname)
            batch_num = get_batch_num(ep, self.batch_size)
            feed_dict_train = {self.X: ep, self.Y: lb}

            # with AE
            # for epoch in range(training_epoch):
            #    total_cost = 0
            #    for i in range(batch_num):
            #        batch_ep = get_batch(ep, batch_size, i)
            #        _, batch_cost = sess.run([optimizer, cost], feed_dict={X: batch_ep})
            #        total_cost += batch_cost
            #    print(f'[AE] Epoch: {epoch+1} & Avg_cost = {total_cost/batch_num}')
            # print(f'Test Reconstruction Cost: {sess.run(cost, feed_dict={X: test_x})}')
            # print(f'Train Reconstruction Cost: {sess.run(cost, feed_dict={X: ep})}')

            epochs = []
            costs = []
            hidden_data = []
            summary = {}
            hidden_data_epoch = {}
            for epoch in range(self.c_training_epoch):
                if epoch % 10 == 0:
                    # hidden_data_epoch['~'].shape = (data_size, hidden_feature_size)
                    encoder1_hidden, encoder2_hidden, encoder3_hidden, encoder4_hidden, encoder5_hidden, c_sigmoid_hidden = sess.run(
                        [self.encoder1, self.encoder2,
                         self.encoder3, self.encoder4, self.encoder5, self.c_sigmoid], feed_dict=feed_dict_train)
                    hidden_data_epoch['epoch'] = epoch + 1
                    hidden_data_epoch['X'] = digitize_tolist_dist(ep)
                    hidden_data_epoch['Y'] = lb
                    hidden_data_epoch['encoder1'] = digitize_tolist_(encoder1_hidden, 1024)
                    hidden_data_epoch['encoder2'] = digitize_tolist_(encoder2_hidden, 1024)
                    hidden_data_epoch['encoder3'] = digitize_tolist_(encoder3_hidden, 1024)
                    hidden_data_epoch['encoder4'] = digitize_tolist_(encoder4_hidden, 1024)
                    hidden_data_epoch['encoder5'] = digitize_tolist_(encoder5_hidden, 1024)
                    hidden_data_epoch['c_sigmoid'] = digitize_tolist(c_sigmoid_hidden)
                    hidden_data.append(hidden_data_epoch)

                print(f'Train Accuracy: {sess.run(self.accuracy * 100, feed_dict={self.X: ep, self.Y: lb})}')
                summary["accuracy"] = sess.run(self.accuracy * 100, feed_dict={self.X: ep, self.Y: lb})
                epochs.append(epoch + 1)
                total_cost = 0
                for i in range(batch_num):
                    batch_ep = get_batch(ep, self.batch_size, i)
                    batch_lb = get_batch(lb, self.batch_size, i)
                    # _, batch_cost = sess.run([self.c_optimizer, self.c_loss], feed_dict={self.X: batch_ep, self.Y: batch_lb})
                    batch_cost = sess.run(self.c_loss, feed_dict={self.X: batch_ep, self.Y: batch_lb})
                    total_cost += batch_cost
                print(f'[Classifier] Epoch: {epoch + 1} & Avg_cost = {total_cost / batch_num}')
                costs.append(total_cost / batch_num)
                summary["loss"] = total_cost / batch_num

            if not os.path.exists(extra_dir):
                os.mkdir(extra_dir)
            with open(extra_dir + "/hidden.pkl", "wb") as f:
                pickle.dump(hidden_data_epoch, f)
            with open(extra_dir + "/summary.pkl", "wb") as f2:
                pickle.dump(summary, f2)
            # sio.savemat(extra_dir + "/hidden.mat", hidden_data_epoch)
            # sio.savemat(extra_dir + "/summary.mat", summary)

            return hidden_data_epoch, summary


print("main")
save_all = False

if save_all:
    eps = []
    lbs = []
    for dataset in [0,1,5]:
    # for dataset in [0]:
        if not os.path.exists(f'./dataset{dataset}'):
            os.mkdir(f'./dataset{dataset}')
        print(f'dataset{dataset}')
        ep_tot, lb_tot = load_data(f'dataset{dataset}_parsed.mat', is_total=True)
        eps.append(ep_tot)
        lbs.append(lb_tot)

print("loading...")
# if os.path.exists('dat_tot.pkl'):
#     with open('dat_tot.pkl','rb') as f:
#         data = pickle.load(f)
#         ep_tots = data['ep'].reshape((data['ep'].shape[0],-1))
#         lb_maxrel_tot = data['lb_maxrel']
#         lb_pmb28_tot = data['lb_pmb28']
#         lb_pmb37_tot = data['lb_pmb37']
#         lb_act_tot = data['lb_act']
# else:
#     ep_tots, lb_maxrel_tot, lb_pmb28_tot, lb_pmb37_tot, lb_act_tot = load_data_labels(
#         './dat_sub/dataset_total.mat')  # original2
# lb_tots = []
# lb_tots.append(lb_maxrel_tot)
# lb_tots.append(lb_pmb28_tot)
# lb_tots.append(lb_pmb37_tot)
# lb_tots.append(lb_act_tot)


ep_tots = []
lb_tots = []
strings_ = "./logs4_"+datetime.today().strftime('%Y%m%d-%H%M') +"/"
# for subi in [0,1,2,4,5,6,7,8,9,10,11,12,13,14,16,18,19,21,24,25,26,27,28,30]:
for subi in range(33):
    ep_tots_, lb_maxrel_tot, lb_pmb28_tot, lb_pmb37_tot, lb_act_tot = load_data_labels(
        './dat_sub/sub{0}.mat'.format(subi+1))  # original2
    ep_tots_ = (ep_tots_.reshape(ep_tots_.shape[0],16,-1))[:,:4,:]
    ep_tots_ = ep_tots_.reshape(ep_tots_.shape[0],-1)
    if len(ep_tots) == 0:
        ep_tots = ep_tots_
    else:
        ep_tots = np.concatenate((ep_tots, ep_tots_),axis=0)
    if len(lb_tots) == 0:
        lb_tots.append(lb_maxrel_tot)
        lb_tots.append(lb_pmb28_tot)
        lb_tots.append(lb_pmb37_tot)
        lb_tots.append(lb_act_tot)
    else:
        lb_tots[0]=np.concatenate((lb_tots[0],lb_maxrel_tot),axis=0)
        lb_tots[1]=np.concatenate((lb_tots[1],lb_pmb28_tot),axis=0)
        lb_tots[2]=np.concatenate((lb_tots[2],lb_pmb37_tot),axis=0)
        lb_tots[3]=np.concatenate((lb_tots[3],lb_act_tot),axis=0)



strings_="./logs4cnn_"  +datetime.today().strftime('%Y%m%d-%H%M')+ "/"
strings_ = "./logs4cnn_20211016_notsave/"

kf = KFold(n_splits=10, shuffle=False)
for lbi in [0]:
    lb_tot = lb_tots[lbi]
    lb1idx = np.where(lb_tot == 0)[0]
    lb2idx = np.where(lb_tot == 1)[0]
    minidx=min(lb1idx.shape[0],lb2idx.shape[0])
    lb1idx = lb1idx[:minidx]
    lb2idx = lb2idx[:minidx]
    lb_tot = np.concatenate((lb_tot[lb1idx],lb_tot[lb2idx]))
    ep_tot = np.concatenate((ep_tots[lb1idx],ep_tots[lb2idx]),axis=0)


    c = -1/(np.sqrt(2)*scipy.special.erfcinv(3/2))
    mad_ = c*np.median(np.abs(ep_tot-np.median(ep_tot,axis=1).reshape(-1,1)),axis=1)
    ep_tot = ep_tot[np.where(mad_<3)[0]]
    lb_tot = lb_tot[np.where(mad_<3)[0]]


    np.random.seed(2020)
    index = np.random.permutation(ep_tot.shape[0])
    ep_tot = ep_tot[index]
    lb_tot = lb_tot[index]

    kf.get_n_splits(lb_tot)
    cv = 0

    print("main2")
    for train_ind, test_ind in kf.split(lb_tot):
        ep, lb = ep_tot[train_ind], lb_tot[train_ind]
        test_x, test_y = ep_tot[test_ind], lb_tot[test_ind]
        ep = ep.reshape(ep.shape[0],4,121,41)
        test_x = test_x.reshape(test_x.shape[0],4,121,41)
        temp1 =np.concatenate((ep[:,0,:,:],ep[:,1,:,:]), axis=1)
        temp2 =np.concatenate((ep[:,2,:,:],ep[:,3,:,:]), axis=1)
        ep = np.concatenate((temp1,temp2),axis=2).reshape((ep.shape[0],121*2,41*2,1))

        temp1 =np.concatenate((test_x[:,0,:,:],test_x[:,1,:,:]), axis=1)
        temp2 =np.concatenate((test_x[:,2,:,:],test_x[:,3,:,:]), axis=1)
        test_x = np.concatenate((temp1,temp2),axis=2).reshape((test_x.shape[0],121*2,41*2,1))



        cv += 1

        if cv > 0:
            network = networks(ep_tot)
            network.init_net()
            # acc = []
            # loss = []
            infoxinfo = []
            epoch = []
            summary = {}
            if not os.path.exists('./cv{0}'.format(cv)):
                os.mkdir('./cv{0}'.format(cv))
            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.900)

            with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
                saver = tf.train.Saver(tf.global_variables())
                # tf.summary.scalar('loss', network.c_loss)
                # tf.summary.scalar('accuracy', network.accuracy)
                # merged = tf.summary.merge_all()
                train_summary_path, val_summary_path = strings_+ f'label{lbi+1}' + "/train",strings_+ f'label{lbi+1}' + "/test"
                train_writer, val_writer = tf.summary.FileWriter(train_summary_path), tf.summary.FileWriter(val_summary_path)

                # train autoencoder
                # ckpt = tf.train.get_checkpoint_state('./model')
                # if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
                #  saver.restore(sess, ckpt.model_checkpoint_path)
                # else:
                #  sess.run(tf.global_variables_initializer())
                sess.run(tf.global_variables_initializer())
                batch_size = network.batch_size
                batch_num = get_batch_num(ep, batch_size)
                batch_num_test = get_batch_num(test_x, batch_size)
                # feed_dict_train = {network.X: ep, network.Y: lb.reshape(-1,1)}
                feed_dict_val = {network.X: test_x, network.Y: test_y.reshape(-1,1)}

                # summary_total = []
                # for dataset in [0, 5]:
                #     summary_total.append({"accuracy": [], "loss": [],
                #                           "epochs": []})  # "information": [], "accuracy": [], "loss": [], "epochs": []})
                epochs = []
                costs = []
                for epoch in tqdm(range(network.c_training_epoch)):

                    if save_all:
                        # orignal dataset 으로부터 뽑아내는 부분
                        if (epoch + 1) == 1:
                            print('#' * 30 + 'original dataset')
                            # train data
                            # acc1, loss1 = network.return_hidden_original(cv, epoch, 1, ep, lb, strings_)
                            # test data
                            acc2, loss2 = network.return_hidden_original(cv, epoch, 2, test_x, test_y, strings_)

                        # dataset 으로부터 뽑아내는 부분
                        if ((epoch + 1) == 1) and (cv == 1):
                            for dataset_idx in range(3):
                                dataset_set = [0, 1, 5]
                                dataset = dataset_set[dataset_idx]
                                print('#' * 30 + f'dataset{dataset}')
                                acc, loss = network.return_hidden(cv, epoch, dataset_idx, eps[dataset_idx],
                                                                  lbs[dataset_idx], strings_)


                    total_cost = 0
                    total_acc = 0
                    ####part
                    for i in range(batch_num):
                        batch_ep = get_batch(ep, batch_size, i)
                        batch_lb = get_batch(lb, batch_size, i)
                        _, batch_cost, batch_acc = sess.run([network.c_optimizer, network.c_loss, network.accuracy],
                                                            feed_dict={network.X: batch_ep,
                                                                       network.Y: batch_lb.reshape(-1, 1)})
                        total_cost += batch_cost
                        total_acc += batch_acc * batch_ep.shape[0]
                    if (epoch + 1) % 50 == 0:
                        print(f'[Classifier] Epoch: {epoch + 1} & Avg_cost = {total_cost / batch_num}')
                    costs.append(total_cost / batch_num)

                    # tf.summary.scalar('loss', total_cost / batch_num)
                    # tf.summary.scalar('accuracy', batch_acc)
                    summary_loss = tf.Summary(value=[tf.Summary.Value(tag="loss", simple_value=total_cost / batch_num)])
                    summary_acc = tf.Summary(value=[tf.Summary.Value(tag="acc", simple_value=total_acc / lb.shape[0])])
                    train_accss = total_acc / lb.shape[0]
                    train_writer.add_summary(summary_loss, global_step=epoch + 1)
                    train_writer.add_summary(summary_acc, global_step=epoch + 1)
                    train_writer.flush()

                    total_cost = 0
                    total_acc = 0
                    ####part
                    for i in range(batch_num_test):
                        batch_ep = get_batch(test_x, batch_size, i)
                        batch_lb = get_batch(test_y, batch_size, i)
                        batch_cost, batch_acc = sess.run([network.c_loss, network.accuracy],
                                                            feed_dict={network.X: batch_ep,
                                                                       network.Y: batch_lb.reshape(-1, 1)})
                        total_cost += batch_cost
                        total_acc += batch_acc * batch_ep.shape[0]

                    # tf.summary.scalar('loss', total_cost / batch_num)
                    # tf.summary.scalar('accuracy', batch_acc)
                    summary_loss = tf.Summary(value=[tf.Summary.Value(tag="loss", simple_value=total_cost / batch_num_test)])
                    summary_acc = tf.Summary(value=[tf.Summary.Value(tag="acc", simple_value=total_acc / test_y.shape[0])])
                    test_accss = total_acc / test_y.shape[0]
                    test_losss = total_cost / batch_num
                    val_writer.add_summary(summary_loss, global_step=epoch + 1)
                    val_writer.add_summary(summary_acc, global_step=epoch + 1)
                    # merged = tf.summary.merge_all()
                    # val_writer.add_summary(merged, global_step=epoch + 1)
                    val_writer.flush()

                    # train_summary = sess.run(merged, feed_dict=feed_dict_train)
                    # val_summary = sess.run(merged, feed_dict=feed_dict_val)


                    # train_writer.add_summary(train_summary, global_step=epoch + 1)
                    # val_writer.add_summary(val_summary, global_step=epoch + 1)


                    # train_writer.flush()
                    # val_writer.flush()
                    if (epoch + 1) % 50 == 0:
                        print(f'Test Accuracy: {test_accss}')
                        print(f'Train Accuracy: {train_accss}')
                        print(f'[Classifier] Epoch: {epoch + 1} & Test_cost = {test_losss}')
                        # print(f'Test Accuracy: {sess.run(network.accuracy * 100, feed_dict={network.X: test_x, network.Y: test_y.reshape(-1,1)})}')
                        # print(f'Train Accuracy: {sess.run(network.accuracy * 100, feed_dict={network.X: ep, network.Y: lb.reshape(-1,1)})}')
                        # cost_test = sess.run([network.c_loss], feed_dict={network.X: test_x, network.Y: test_y.reshape(-1,1)})
                        # print(f'[Classifier] Epoch: {epoch + 1} & Test_cost = {cost_test}')

                    # if (epoch + 1) == 100:
                    #     if not os.path.exists(strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1)):
                    #         os.makedirs(strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1))
                    #     saver.save(sess,strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1) + '/dnn.ckpt')

                    if (epoch + 1) % 1000 == 0:
                        if not os.path.exists(strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1)):
                            os.makedirs(strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1)  + str(epoch + 1))
                        saver.save(sess, strings_ + '/cv{0}/label{1}/model_'.format(cv,lbi+1) + str(epoch + 1) + '/dnn.ckpt')

                    if save_all:
                        if (epoch + 1) % 50 == 0:
                            # acc1, loss1 = network.return_hidden_original(epoch, eps[dataset_idx], lbs[dataset_idx])

                            # train data
                            # acc1, loss1 = network.return_hidden_original(cv, epoch, 1, ep, lb, strings_)
                            # test data
                            acc2, loss2 = network.return_hidden_original(cv, epoch, 2, test_x, test_y, strings_)

                            # original dataset 에 대해서 들어가야하고
                            # epoch 들어가야하고
                            # data 전부를 넣고 train test idx를 넣어야한다
                            # training data test data 구분해서 넣는다
                            # training test 다 들어가는데 어떻게하냐
                            # training test 둘다 뽑자.
                            # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'

                        # dataset 으로부터 뽑아내는 부분
                        if ((epoch + 1) % 50 == 0) and (cv == 1):
                            for dataset_idx in range(3):
                                dataset_set = [0, 1, 5]
                                dataset = dataset_set[dataset_idx]
                                print('#' * 30 + f'dataset{dataset}')
                                acc, loss = network.return_hidden(cv, epoch, dataset_idx, eps[dataset_idx],
                                                                  lbs[dataset_idx], strings_)
                                # dataset 받고, epoch '/cv{0}'.format(cv) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
                                # with open([파일이름],'wb') with f:
                                #   pickle.dump(f,[])
                                # summary_total[dataset_idx]["information"].append(info)
                                # summary_total[dataset_idx]["accuracy"].append(acc)
                                # summary_total[dataset_idx]["loss"].append(loss)
                                # summary_total[dataset_idx]["epochs"].append(epoch)
                                # try:
                                #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
                                # except:
                                #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
                                #         pickle.dump(summary_total[dataset_idx])
                    # orignal dataset 으로부터 뽑아내는 부분
                    # if (epoch + 1) % 10 == 0:

                        #acc1, loss1 = network.return_hidden_original(epoch, eps[dataset_idx], lbs[dataset_idx])

                        # # train data
                        # acc1, loss1 = network.return_hidden_original(cv, epoch, 1, ep, lb)
                        # # test data
                        # acc2, loss2 = network.return_hidden_original(cv, epoch, 2, test_x, test_y)

                        # original dataset 에 대해서 들어가야하고
                        # epoch 들어가야하고
                        # data 전부를 넣고 train test idx를 넣어야한다
                        # training data test data 구분해서 넣는다
                        # training test 다 들어가는데 어떻게하냐
                        # training test 둘다 뽑자.
                        # '/cv{0}'.format(cv) + '/original/hidden' + str(epoch) + {'_train' or '_test'} + '.pkl'

                    # dataset 으로부터 뽑아내는 부분
                    # if ((epoch + 1) % 10 == 0) and (cv == 1):
                    #     for dataset_idx in range(2):
                    #         dataset_set = [0, 5]
                    #         dataset = dataset_set[dataset_idx]
                    #         print('#' * 30 + f'dataset{dataset}')
                    #         acc, loss = network.return_hidden(cv, epoch, dataset_idx, eps[dataset_idx], lbs[dataset_idx])
                            # dataset 받고, epoch '/cv{0}'.format(cv) + f'/dataset{dataset}/hidden' + str(epoch) + '.pkl'
                            # with open([파일이름],'wb') with f:
                            #   pickle.dump(f,[])
                            # summary_total[dataset_idx]["information"].append(info)
                            # summary_total[dataset_idx]["accuracy"].append(acc)
                            # summary_total[dataset_idx]["loss"].append(loss)
                            # summary_total[dataset_idx]["epochs"].append(epoch)
                            # try:
                            #     sio.savemat(f'./dataset{dataset}/summary_total.mat', summary_total[dataset_idx])
                            # except:
                            #     with open(f'./dataset{dataset}/summary_total.pkl', 'rb') as f:
                            #         pickle.dump(summary_total[dataset_idx])



